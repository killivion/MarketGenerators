{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option Pricing Logic for all One-Time Trained YFinance Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to create the required files first in other notebook(s) before evaluating them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fbm\n",
    "!pip install yfinance==0.1.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set working directory to MarketGenerators folder\n",
    "# If you are working on LRZ servers, create the folder \"MarketGenerators\" and then specify something like\n",
    "path = \"/dss/dsshome1/02/YOUR_LRZ_USER_NAME/MarketGenerators\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.evaluate import metrics as m\n",
    "from src.data.make_dataset import DataLoader\n",
    "import src.helper.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory\n",
    "base_dir = \"numerical_results\"\n",
    "# Define the folders of interest\n",
    "model_folders = [\"GBM\", \"Kou_Jump_Diffusion\"]\n",
    "# Define the target subfolders\n",
    "folders_to_exclude = [\"n-in=2Y\", \"n-in=9Y\", \"n-in=20Y\", \"n-in=50Y\", \"n-in=99Y\", \"n-in=999Y\"]\n",
    "# Define the target subfolders\n",
    "target_subfolders = [\"CWGAN\", \"GMMN\", \"RCGAN\", \"SigCWGAN\", \"TimeGAN\"]\n",
    "option_types_K = [\"European\", \"Asian\"]\n",
    "n_days_list = [5, 10, 21, 252]\n",
    "# Parameters\n",
    "S0 = 1\n",
    "t = 0\n",
    "# Only evaluate lookback option at time T ==> Make grid size large\n",
    "look_back_grid_size = 1000000\n",
    "# Each (disjoint) batch consists of 1,000 paths\n",
    "n_batches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nDays in n_days_list:\n",
    "    K_grid = np.linspace(0.95, 1.1, 4) if nDays >= 21 else np.linspace(1, 1.08, 3)\n",
    "    settings = [f\"{metric}_K={K:.2f}\" for metric in option_types_K for K in K_grid]\n",
    "    settings.append(\"Lookback\")\n",
    "    T = nDays / 252\n",
    "    # Initialize empty last specification\n",
    "    last_spec = \"\"\n",
    "\n",
    "    # Loop through each model folder\n",
    "    for model_folder in model_folders:\n",
    "        print(f\"Start evaluating all {model_folder}-based models for {nDays} days maturity and {n_batches} batches...\")\n",
    "        model_path = os.path.join(base_dir, model_folder)\n",
    "\n",
    "        # Traverse the directory tree\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            gen_model = os.path.basename(root)\n",
    "\n",
    "            if gen_model in target_subfolders:\n",
    "                # Skip retrained folders\n",
    "                nYearsFolderName = root.split(\"/\")[3]\n",
    "                if nYearsFolderName in folders_to_exclude:\n",
    "                    continue\n",
    "\n",
    "                # Read the relevant npy files\n",
    "                generated_file = os.path.join(root, \"generated_returns_rescaled.npy\")\n",
    "                input_file = os.path.join(root, \"input_returns_unscaled.npy\")\n",
    "                if os.path.exists(generated_file) and os.path.exists(input_file):\n",
    "                    input_prices_df, generated_prices_df, generated_returns = utils.load_input_and_generated_returns(\n",
    "                        input_file, generated_file, nDays, T\n",
    "                    )\n",
    "                    input_model, model_spec = root.split(\"/\")[1:3]\n",
    "                    # TimeGAN is first folder ==> New evaluation\n",
    "                    # THIS MAY CHANGE AND NEEDS TO BE ADJUSTED IF YOU ADD NEW/OTHER MODELS\n",
    "                    if gen_model == \"TimeGAN\":\n",
    "                        results_call = {setting: {gan: [] for gan in target_subfolders + [\"Input\"]} for setting in settings}\n",
    "                        results_put = {setting: {gan: [] for gan in target_subfolders + [\"Input\"]} for setting in settings}\n",
    "                        model_desc = \"/\".join(root.split(\"/\")[1:5])\n",
    "                        pairs = model_spec.split(\"_\")\n",
    "                        params = {key: float(value) for key, value in (pair.split(\"=\") for pair in pairs)}\n",
    "                        recalculate_input = True\n",
    "                        print(f\"   Evaluating model {model_desc}...\")\n",
    "\n",
    "                        if model_spec != last_spec:\n",
    "                            last_spec = model_spec\n",
    "                            if input_model != \"GBM\":\n",
    "                                # create data for approximate exact price (1,000,000 paths) in the first run\n",
    "                                # fix lambda naming:\n",
    "                                if \"lambda\" in params:\n",
    "                                    params[\"lambda_\"] = params.pop(\"lambda\")\n",
    "                                n_approx = 1000000\n",
    "                                params[\"n\"] = n_approx\n",
    "                                params[\"T\"] = T\n",
    "                                params[\"n_points\"] = nDays + 1\n",
    "                                params[\"S0\"] = S0\n",
    "                                dataloader = DataLoader(method=root.split(\"/\")[1], params=params, seed=314)\n",
    "                                print(f\"      ...Generating {n_approx} paths of {input_model} model...\")\n",
    "                                approx_df = dataloader.create_dataset(output_type=\"DataFrame\")\n",
    "                                approx_df_lookback = approx_df.iloc[:10000, :]\n",
    "                                last_spec = model_spec\n",
    "\n",
    "                            european_engine, asian_engine, lookback_engine = utils.initialize_all_option_engines(\n",
    "                                input_prices_df, generated_prices_df, T, t=t, S0=S0, approx_exact=(input_model != \"GBM\")\n",
    "                            )\n",
    "\n",
    "                        input_spec = \"-\".join(root.split(\"/\")[1:5]).replace(\".\", \",\") + f\"_nDays={nDays}\"\n",
    "\n",
    "                        # Get respective path metrics\n",
    "                        if input_model == \"GBM\":\n",
    "                            # To do: read the sigma and mu values dynamically (not important for now)\n",
    "                            approx_df = None\n",
    "                            approx_df_lookback = None\n",
    "                            european_engine.sigma = params[\"sigma\"]\n",
    "                            european_engine.r = params[\"mu\"]\n",
    "                            asian_engine.sigma = params[\"sigma\"]\n",
    "                            asian_engine.r = params[\"mu\"]\n",
    "                            lookback_engine.sigma = params[\"sigma\"]\n",
    "                            lookback_engine.r = params[\"mu\"]\n",
    "                        else:\n",
    "                            with utils.Capturing([]) as summary_output:\n",
    "                                # Write summary to txt\n",
    "                                european_engine.r = m.print_basic_non_gbm_metrics(\n",
    "                                    n_periods=T,\n",
    "                                    annualization_factor=252,\n",
    "                                    ground_paths_df=input_prices_df,\n",
    "                                    recovered_paths_df=generated_prices_df,\n",
    "                                    approx_df=approx_df,\n",
    "                                    return_threshold=0.03\n",
    "                                )\n",
    "                            asian_engine.r = european_engine.r\n",
    "                            lookback_engine.r = european_engine.r\n",
    "\n",
    "                    print(f\"         Calculating option prices for {gen_model} paths\")\n",
    "                    for batch in range(n_batches):\n",
    "                        european_engine.gen_paths_df = generated_prices_df.iloc[batch * 1000:(batch + 1) * 1000, :]\n",
    "                        asian_engine.gen_paths_df = generated_prices_df.iloc[batch * 1000:(batch + 1) * 1000, :]\n",
    "                        lookback_engine.gen_paths_df = generated_prices_df.iloc[batch * 1000:(batch + 1) * 1000, :]\n",
    "                        european_engine.calc_all_K(K_grid, approx_df=approx_df, recalculate_input=recalculate_input)\n",
    "                        asian_engine.calc_all_K(K_grid, approx_df=approx_df, recalculate_input=recalculate_input)\n",
    "                        # Use less data for runtime reasons for approx_df for lookback options logic\n",
    "                        lookback_engine.calc_all_T(grid_size=look_back_grid_size, approx_df=approx_df_lookback, recalculate_input=recalculate_input)\n",
    "\n",
    "                        results_call, results_put = utils.fill_results(\n",
    "                            european_engine, asian_engine, lookback_engine, results_call, results_put,\n",
    "                            gen_model, recalculate_input=recalculate_input, has_input_dev=True\n",
    "                        )\n",
    "                        recalculate_input = False\n",
    "\n",
    "                    if gen_model == \"GMMN\":\n",
    "                        # THIS NEEDS TO BE CHANGED TO THE LAST MODEL IN THE TREE IF SOMETHING CHANGED\n",
    "                        print(\"      Saving results...\")\n",
    "                        relevant_dir = \"/\".join(root.split(\"/\")[:-2])\n",
    "                        utils.save_stat_analysis_to_csv(\n",
    "                            settings, results_put, lookback_engine, european_engine, asian_engine,\n",
    "                            relevant_dir, target_subfolders, nDays, \"put\", one_time_trained=True\n",
    "                        )\n",
    "                        utils.save_stat_analysis_to_csv(\n",
    "                            settings, results_call, lookback_engine, european_engine, asian_engine,\n",
    "                            relevant_dir, target_subfolders, nDays, \"call\", one_time_trained=True\n",
    "                        )\n",
    "                else:\n",
    "                    print(\"No file found yet.\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
